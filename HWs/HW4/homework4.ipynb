{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Bayesian inference with MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a simple case of Bayesian inference for the unknown mean parameter of a Normal distribution. A series of values $\\boldsymbol{X}^{(n)} = X_1, X_2, \\dots X_n$ is sampled **independently** from a Normal distribution and we know its variance in advance, but are unsure about the mean. We assign a prior distribution to this unknown mean and would like to infer its posterior. We are also interested in predicting future observations $X$, so we would like to obtain its posterior predictive distribution as well.\n",
    "$$\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$$\n",
    "$$X_i | \\mu \\sim \\mathcal{N}(\\mu, \\sigma^2), \\; i=1 \\dots n.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(31337)\n",
    "\n",
    "true_mu = 3.0\n",
    "true_std = 0.3\n",
    "N = 20\n",
    "\n",
    "prior_mu = 0.\n",
    "prior_std = 1.\n",
    "\n",
    "X = np.random.normal(true_mu, true_std, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: implement analytical posterior (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\mu | \\boldsymbol{X}^{(n)}) = \\frac{p(\\boldsymbol{X}^{(n)} | \\mu) p(\\mu)}{p(\\boldsymbol{X}^{(n)})}, \\label{eq1} \\tag{1}$$\n",
    "$$p(\\boldsymbol{X}^{(n)}) = \\int p(\\boldsymbol{X}^{(n)}, \\mu) d\\mu$$\n",
    "For our simple case it is actually possible to derive the exact posterior distibution of the mean analytically. Posterior distiribution of $\\mu$ will also be Normal, but with different parameters that will depend on the observed sample and prior parameters. Refer to https://en.wikipedia.org/wiki/Conjugate_prior for the exact formulas and implement them in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_posterior_pdf(x, samples, sigma, mu_0, sigma_0):\n",
    "    \"\"\"\n",
    "    Probability density of the analytical posterior for the Normal model with known variance\n",
    "    \n",
    "    :param x: point(s) where to evaluate the density\n",
    "    :param samples: observed samples\n",
    "    :param sigma: known standard deviation of the observations\n",
    "    :param mu_0: mean of the prior\n",
    "    :param sigma_0: standard deviation of the prior\n",
    "    :return: p(mu | samples)(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code code to derive the parameters\n",
    "    \n",
    "    mu_p = None\n",
    "    sigma_p = None\n",
    "        \n",
    "    \n",
    "    return scipy.stats.norm(mu_p, sigma_p).pdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting analytical posterior for the given observed sample, standard deviation of $X_i$ and prior parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Metropilis-Hastings algorithm (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more complex models it is usually infeasible to derive the exact formula for the posterior density. In the equation (1), the numerator contains easy to evaluate terms: prior and likelihood, while the denominator is rather hard to find. This means that we know our posterior only up to a constant multiplicative term. We will have to resort to approximations. Previously in the course we have studied methods to estimate the unknown density from samples. If we could sample from this unknown posterior, we might be able to approximate in using a histogram or a kernel method.\n",
    "\n",
    "This is precisely the goal of MCMC methods. We will construct a Markov chain that will eventually produce samples from our distribution of interest - the posterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov chain is a sequence of random variables $Y_1, Y_2, \\dots$ with the following property:\n",
    "$$p(Y_k | Y_{k-1}, Y_{k-2}, \\dots, Y_2, Y_1) = p(Y_k | Y_{k-1}),$$\n",
    "wich means that subsequent values ($Y_k$) do not depend on the whole history of the sequence if the previous value ($Y_{k-1}$) is given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to start with our chain from the prior distribution $Y_1 \\sim p(\\mu)$ and eventually converge to the posterior $Y_k \\sim p(\\mu | \\boldsymbol{X}^{(n)})$. In order to achieve this, we need to define the transition probabilities $p(Y_k | Y_{k-1})$ in a special way. Intuitevely, it will depend on the prior and posterior in some way and there may be different possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will describe one way to do it, namely Metropilis-Hastings algorithm with a random walk proposal. It was introduced in lecture 10, slides 19-22. We will use it to draw a sequence of samples from a special markov chain that satisfies our requirements.\n",
    "\n",
    "Let $T$ be the number of steps we decided to take from the chan. For our example model and using our notation the algorithm outilines as follows:\n",
    "\n",
    "1. Start by sampling $Y_1$ from the prior $p(\\mu)$\n",
    "2. Repeat for each $k$ form 1 to T:\n",
    "3. Construct the next sample $Y_{k+1}$ given the previous sampled value $Y_k$:\n",
    "4. $Y_{k+1} = Y_{k} + Z_{k+1}$, where $Z_{k+1} \\sim \\mathcal{N}(0, \\sigma_{step}^2)$ (we take a sample)\n",
    "5. We need to decide whether to make the transition $Y_k \\rightarrow Y_{k+1}$ (accept or reject our *proposed* new value $Y_{k+1}$):\n",
    "6. Accept the new sample with probability $\\min \\left\\{1, \\frac{p(\\boldsymbol{X}^{(n)} | \\mu = Y_{k+1}) p(\\mu = Y_{k+1})}{p(\\boldsymbol{X}^{(n)} | \\mu = Y_{k}) p(\\mu = Y_{k})}\\right\\}$, otherwise let $Y_{k+1} = Y_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our acceptance probability equals the ratio of the **posterior** probabilities: $\\frac{p(\\mu=Y_{k+1} | \\boldsymbol{X}^{(n)})}{p(\\mu=Y_k | \\boldsymbol{X}^{(n)})}$, since the denominator in formula (1) does not depend on $\\mu$. This allows us to skip the hard part of taking the integral. \n",
    "\n",
    "*You may need to tune the $\\sigma_{step}$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptance_prob(y_new, y_old, samples, sigma, mu_0, sigma_0):\n",
    "    \"\"\"\n",
    "    Calculate acceptance probability in Metropilis-Hastings step for the Normal model with known variance\n",
    "    :param y_new: proposed value (corresponds to Y_{k+1})\n",
    "    :param y_old: previous value (corresponds to Y_k)\n",
    "    :param samples: observed data (X^(n))\n",
    "    :param sigma: known standard deviation of the observations\n",
    "    :param mu_0: mean of the prior\n",
    "    :param sigma_0: standard deviation of the prior\n",
    "    :return: p, acceptance probability\n",
    "    \"\"\"\n",
    "\n",
    "    # Your code here\n",
    "    r = 0\n",
    "    p = min(1, r)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "\n",
    "sigma_step = 0.1\n",
    "\n",
    "Y = np.empty(T)\n",
    "\n",
    "Y[0] = np.random.normal(prior_mu, prior_std)\n",
    "\n",
    "for i in tqdm(range(1, T)):\n",
    "    # Your code here\n",
    "    # Z = ...\n",
    "    # Y_next = ...\n",
    "    \n",
    "    # if np.random.rand() < acceptance_prob(...):\n",
    "    #    Y[i] = ...\n",
    "    # else:\n",
    "    #    Y[i] = ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Visualize the results (1 point)\n",
    "We have obtained a single trajectory of our Markov chain. The tsarting portion the obtained values may be different from the samples of the actual posterinir. After the so-called 'burn-in' or 'warmup' phase the distribution should become stationary and connverge to the true posterior. Time spent before this convergence occurs is usually called *mixing time*, and it is hard to estimate in practical situations.\n",
    "\n",
    "1. Plot your trajectory using line chart: x-axis will be the timestep number and y-axis the sampled value\n",
    "\n",
    "2. Plot the acceptance probability: how many proposed steps were accepted on average at given timestep (use sliding window or exponential moving average). Does it change over time? (You may need to save additional info during sampling in the previous part)\n",
    "\n",
    "3. When do you think the 'mixing' occured? (Approximately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4: Approximate posterior (1 points)\n",
    "\n",
    "Now we have determined the part of our trajectory that converjed to our target distribution (you can use your guess from the previous task or just designate first half the trajectory for mixing). We will work with the remaing part to approximate the posterior.\n",
    "\n",
    "1. Plot the histogram and kernel density estimator of the posterior using the samples from your MCMC run\n",
    "2. Unfortunately, the samples that we obtained are not independent: due to the nature of our proposal step they are correlated. We can try to aleviate this effect by dropping some of the samples. Use only every 3rd (or 4th, 5th) obtained sample and display it on the same plot. Is it different?\n",
    "3. Add theoretical posterior from part 1 to the same plot. Is it close to our estimate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5: Posterior predictive distribution (1 bonus point)\n",
    "\n",
    "Now that we have obtained the estimate of the posterior, we can turn to a more practical task: predicting new values $X_{n+1}$. To do it Bayesian fashoin, we employ posterior predictive distribution:\n",
    "$$p(X_{n+1} | \\boldsymbol{X}^{(n)}) = \\int p(X_{n+1}, \\mu | \\boldsymbol{X}^{(n)}) d \\mu = \\int p(X_{n+1} | \\mu, \\boldsymbol{X}^{(n)}) p(\\mu | \\boldsymbol{X}^{(n)})d \\mu = \\int p(X_{n+1} | \\mu) p(\\mu | \\boldsymbol{X}^{(n)})d \\mu$$\n",
    "\n",
    "Similar to part 1, for our simple model this distribution can also be derived analyticaly and is described on the same Wiki page.\n",
    "\n",
    "1. Implement analytical posterior predictive distribution for our model and plot it\n",
    "2. Using decorrelated samples from the posterior, obtained in the previous part, approximate the posterior predictive distribution (with histogram or KDE) and add in to the plot. Is it close to the analytocal result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWSj-xmfGjPK"
   },
   "source": [
    "# Task 2. Substitution Cipher and Markov Chain Monte Carlo (MCMC). (5 points)\n",
    "\n",
    "In this task we will decrypt data that was scrambled using a Substitution Cipher. We assume that encryption key is unknown and we want to decrypt the data and read the code using recovered decryption key. [Introduction from here](http://statweb.stanford.edu/~cgates/PERSI/papers/MCMCRev.pdf) gives reference to original task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-TxsSUSGjPP"
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWSj-xmfGjPK"
   },
   "source": [
    "As verification we will take a piece from \"Alice's adventures in Wonderland\". We scramble data with a random encryption key, which we forgot after encrypting, and we would like to decrypt this encrypted text using MCMC Chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_text = \"\"\"\n",
    "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
    "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n",
    "There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before see a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n",
    "In another moment down went Alice after it, never once considering how in the world she was to get out again.\n",
    "The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 26 letters of English alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = string.ascii_lowercase\n",
    "characters_dict = {char : c for c, char in enumerate(characters, start=1)}\n",
    "m = len(characters) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random encryption key.\n",
    "\n",
    "Here are functions that will be used to encrypt/decrypt text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text_to_encode, characters_dict):\n",
    "    \"\"\"This function turns text string into integer sequence using given dictionary.\"\"\"\n",
    "    characters_set = set(characters_dict.keys())\n",
    "    return np.r_[[characters_dict[char] if char in characters_set else 0 for char in text_to_encode.strip().lower()]]\n",
    "\n",
    "def decode_text(text_to_decode, characters):\n",
    "    \"\"\"This function turns integer sequence into text string using given list of characters.\"\"\"\n",
    "    characters_array = np.array([\" \"] + list(characters))\n",
    "    return \"\".join(characters_array[text_to_decode])\n",
    "\n",
    "def apply_cipher(text_as_int_array, cipher):\n",
    "    \"This function applies substitution cipher to integer sequence.\"\n",
    "    return cipher[text_as_int_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate encryption and decryption keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "encryption_indices = np.random.permutation(np.arange(m-1))\n",
    "decryption_indices = np.argsort(encryption_indices)\n",
    "\n",
    "characters_array = np.array(list(characters))\n",
    "encryption_key = \"\".join(characters_array[encryption_indices])\n",
    "decryption_key = \"\".join(characters_array[decryption_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check encoding/decoding functions and encryption/decryption keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encryption_key_encoded = np.r_[0, encode_text(encryption_key, characters_dict)]\n",
    "decryption_key_encoded = np.r_[0, encode_text(decryption_key, characters_dict)]\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "encoded_text = encode_text(text, characters_dict)\n",
    "cipher_text = apply_cipher(encoded_text, encryption_key_encoded)\n",
    "encoded_text = apply_cipher(cipher_text, decryption_key_encoded)\n",
    "decoded_text = decode_text(encoded_text, characters_dict)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encrypt cipher text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_text_encoded = encode_text(plain_text, characters_dict)\n",
    "cipher_text = apply_cipher(plain_text_encoded, encryption_key_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect frequences \n",
    "\n",
    "Collect frequences over large text corpus and from encrypted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_transition_frequences(data, transition_matrix):\n",
    "    \"\"\"For a given integer sequence, which corresponds to some char sequence, \n",
    "       return transitions for adjacent values.\"\"\"\n",
    "    transitions = data.repeat(2)[1:-1].reshape(-1, 2)\n",
    "    for i, j in transitions:\n",
    "        transition_matrix[i, j] += 1\n",
    "    \n",
    "    return transition_matrix\n",
    "\n",
    "def collect_empirical_frequences(filename, characters_dict, m):\n",
    "    \"\"\"Collect frequences over large text corpus, return transition matrix.\"\"\"\n",
    "    transition_matrix = np.zeros((m, m))\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line_encoded = encode_text(line, characters_dict)\n",
    "            if line_encoded.size > 1:\n",
    "                transition_matrix = collect_transition_frequences(line_encoded, transition_matrix)\n",
    "                \n",
    "    return transition_matrix\n",
    "\n",
    "def collect_observed_frequences(cipher_text, characters_dict, m):\n",
    "    \"\"\"Collect frequences over encrypted text, return nonzero indices of \n",
    "       transition matrix for both dimentions and values for those indices.\n",
    "       `values = transition_matrix[indices_1, indices_2]`\"\"\"\n",
    "    transition_matrix = np.zeros((m, m))\n",
    "    transition_matrix = collect_transition_frequences(cipher_text, transition_matrix)\n",
    "    \n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect frequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_frequences = collect_empirical_frequences('war_and_peace.txt', characters_dict, m)\n",
    "observed_frequences = collect_observed_frequences(cipher_text, characters_dict, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjedc-_xGjPO"
   },
   "source": [
    "Our Chain will include states that are permutations of the substitution cipher. Algorithm has following steps:\n",
    "\n",
    "1. Start by picking up a random current state. \n",
    "2. Create a proposal for a new state by swapping two or more random letters in the current state.\n",
    "3. Use a Scoring Function which calculates the score of the current state $Score_{old}$ and the proposed State $Score_{new}$.\n",
    "4. If the score of the proposed state is more than current state, Move to Proposed State.\n",
    "5. Else flip a coin which has a probability of Heads $\\frac{Score_{new}}{Score_{old}}$  . If it comes heads move to proposed State.\n",
    "6. Repeat from Step 2.\n",
    "\n",
    "We want ot reach a steady state where the chain has the stationary distribution of the needed states. This state of chain could be used as a solution.\n",
    "\n",
    "Let's start with implementing steps 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare sampling function (2 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjedc-_xGjPO"
   },
   "source": [
    "\n",
    "\n",
    "To generate a new proposed cipher we randomly select several positions and swap values at those positions. It corresponds to change in seveal mappings of encrypted characters in decrypted ones. Example with 2 swaps.\n",
    "\n",
    "was|now\n",
    "-|-\n",
    "A -> B | A -> B\n",
    "B -> C | B -> C\n",
    "C -> D | C -> A\n",
    "D -> A | D -> D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cipher(cipher, m, size=2):\n",
    "    \"\"\"Swap two or more random positions in cipher.\n",
    "        \n",
    "        cipher, np.array - current mapping from value(int) in encrypted text (index of array cell) into value(int) in decrypted text(value of array cell).\n",
    "        m, int - capacity of used alphabet,\n",
    "        size, int - number of positions to change.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    return new_cipher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare scoring function (2 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "siBVVI9ZGjPP"
   },
   "source": [
    "We want to use a scoring function for each state(Decryption key) which assigns a positive score to each decryption key. This score intuitively should be larger if the encrypted text looks more like actual english, when decrypted using this decryption key. We will check large text and calculate frequences: how many times one character comes after another in a large text like \"War and Peace\".\n",
    "\n",
    "For each pair of characters $\\beta_1$ and $\\beta_2$ (e.g. $\\beta_1$ = A and $\\beta_2$ = B), we let $R(\\beta_1,\\beta_2)$ record the number of times that specific pair(e.g. \"AB\") appears consecutively in the reference text.\n",
    "\n",
    "Similarly, for a considered decryption key $x$, we let $F_x(\\beta_1,\\beta_2)$ record the number of times that\n",
    "pair appears when the cipher text is decrypted using the decryption key $x$.\n",
    "\n",
    "We then Score a particular decryption key $x$ using:\n",
    "\n",
    "$$Score(x) = \\prod R(\\beta_1,\\beta_2)^{F_x(\\beta_1,\\beta_2)}$$\n",
    "    \n",
    "To make life easier with calculations we will calculate $log(Score(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to implement scoring function. As input it takes \n",
    "- `cipher`: mapping between encrypted characters and decrypted characters,\n",
    "- `observed_frequences`: transition matrix for cipher text, matrix representation of $F_x(\\beta_1,\\beta_2)$,\n",
    "- `empirical_frequences`: transition matrix for large text, matrix representation of $R(\\beta_1,\\beta_2)$.\n",
    "\n",
    "Scoring function returns $log(Score(x))$. You need correctly process zero values in transition matrices while calculating the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_cipher(cipher, observed_frequences, empirical_frequences):\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decryption\n",
    "\n",
    "Now we a ready to decrypt cipher text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypting(observed_frequences, empirical_frequences, n_iters, m, step_size, seed, print_it=1000):\n",
    "    \"\"\"This function finds most suited decrypting cipher(1D np.array).\n",
    "        observed_frequences, 2D np.array - transition matrix with frequences for cipher text,\n",
    "        empirical_frequences, 2D np.array - transition matrix with frequences for large text,\n",
    "        n_iters, int - number of MCMC iterations,\n",
    "        step_size, int - number of changes in cipher per one iteration,\n",
    "        seed, int - seed for random generator,\n",
    "        print_it, int - print decrypted text every `print_it` iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1. Start by picking up a random current state. \n",
    "    cipher_old = np.arange(m)\n",
    "    score_cipher_old = score_cipher(cipher_old, observed_frequences, empirical_frequences)\n",
    "    best_state, score = cipher_old, score_cipher_old\n",
    "\n",
    "    for i in tqdm(range(1, n_iters+1)):\n",
    "\n",
    "        # 2. Create a proposal for a new state by swapping two or more random letters in the current state.\n",
    "        cipher_new = generate_cipher(cipher_old, m, size=step_size)\n",
    "\n",
    "        # 3. Use a Scoring Function which calculates the score of the current state $Score_{old}$ and the proposed State $Score_{new}$.\n",
    "        score_cipher_new = score_cipher(cipher_new, observed_frequences, empirical_frequences)\n",
    "        acceptance_probability = np.min((1, np.exp(score_cipher_new - score_cipher_old)))\n",
    "\n",
    "        # 4. If the score of the proposed state is more than current state, Move to Proposed State.\n",
    "        # 5. Else flip a coin which has a probability of Heads $Score_{new}/Score_{old}$. If it comes heads move to proposed State.\n",
    "        if score_cipher_old > score:\n",
    "            best_state, score = cipher_old, score_cipher_old\n",
    "        if acceptance_probability > np.random.uniform(0,1):\n",
    "            cipher_old, score_cipher_old = cipher_new, score_cipher_new\n",
    "        if i % print_it == 0:\n",
    "            print(f\"iter {i}: {decode_text(apply_cipher(cipher_text[0:99], cipher_old), characters)}\")\n",
    "\n",
    "    return best_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decrypt_cipher = decrypting(observed_frequences, empirical_frequences, 10000, m, 4, 345, 1000)\n",
    "\n",
    "print(\n",
    "    f\"\\nDecoded Text: {decode_text(apply_cipher(cipher_text, decrypt_cipher), characters)}\\n\\n\"\n",
    "    f\"MCMC KEY  : {''.join(characters_array[decrypt_cipher[1:]-1])}\\n\"\n",
    "    f\"ACTual KEY: {decryption_key}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning parameters (1 point).\n",
    "\n",
    "For step sizes in range `[2, ..., 7]`(number of pertrubations in newly generated cipher) find number of iteratoins, with `..00` precision(`print_it=100`), which is necessary to make cipher text looks more like english one. Plot obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
