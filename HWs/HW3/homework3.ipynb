{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you need to solve four tasks. The assignment should be uploaded in Jupyter Notebook format(`.ipynb`). Overall, there are 23 points, however, you can get a maximum of 20 points for this assignment( `your_points = min(your_points, 20)`). No bonus points will be transferred to the next assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Non-parametric Density Estimation (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as nla\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib settings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# non-interactive\n",
    "%matplotlib inline\n",
    "\n",
    "#jupyterlab\n",
    "# %matplotlib widget \n",
    "\n",
    "#jupyter-notebook\n",
    "# %matplotlib notebook \n",
    "\n",
    "titlesize = 20\n",
    "labelsize = 16\n",
    "legendsize = labelsize\n",
    "xticksize = 14\n",
    "yticksize = xticksize\n",
    "\n",
    "matplotlib.rcParams['legend.markerscale'] = 1.5     # the relative size of legend markers vs. original\n",
    "matplotlib.rcParams['legend.handletextpad'] = 0.5\n",
    "matplotlib.rcParams['legend.labelspacing'] = 0.4    # the vertical space between the legend entries in fraction of fontsize\n",
    "matplotlib.rcParams['legend.borderpad'] = 0.5       # border whitespace in fontsize units\n",
    "matplotlib.rcParams['font.size'] = 12\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "matplotlib.rcParams['font.serif'] = 'Times New Roman'\n",
    "matplotlib.rcParams['axes.labelsize'] = labelsize\n",
    "matplotlib.rcParams['axes.titlesize'] = titlesize\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=xticksize)\n",
    "matplotlib.rc('ytick', labelsize=yticksize)\n",
    "matplotlib.rc('legend', fontsize=legendsize)\n",
    "\n",
    "matplotlib.rc('font', **{'family':'serif'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will work with data sampled from the mixture of Normal distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed1 = 31337\n",
    "seed2 = 42\n",
    "n_samples = 100\n",
    "\n",
    "f1 = scipy.stats.norm(loc=0, scale=1)\n",
    "f2 = scipy.stats.norm(loc=5, scale=1)\n",
    "p1 = 0.3\n",
    "p2 = 1 - p1\n",
    "\n",
    "Xa = f1.rvs(size=int(p1 * n_samples), random_state=seed1)\n",
    "Xb = f2.rvs(size=int(p2 * n_samples), random_state=seed2)\n",
    "samples = np.concatenate([Xa, Xb])\n",
    "\n",
    "a = -5\n",
    "b = 10\n",
    "\n",
    "x_values = np.linspace(a, b, 1000)\n",
    "binedges = np.linspace(a, b, 10)\n",
    "true_pdf = p1 * f1.pdf(x_values) + p2 * f2.pdf(x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\boldX}{\\boldsymbol{X}}$\n",
    "$\\newcommand{\\xs}[1]{\\boldX^{(#1)}}$\n",
    "$\\newcommand{\\Sum}{\\sum\\limits}$\n",
    "$\\newcommand{\\Int}{\\int\\limits}$\n",
    "$\\newcommand{\\hatp}{\\widehat{p}}$\n",
    "$\\newcommand{\\hatJ}{\\widehat{J}}$\n",
    "$\\newcommand{\\lp}{\\left(}$\n",
    "$\\newcommand{\\rp}{\\right)}$\n",
    "\n",
    "Given a sample $\\boldsymbol{X}^{(n)} = \\boldsymbol{X}_1, \\boldsymbol{X}_2, ..., \\boldsymbol{X}_n \\sim \\; iid \\; p(x)$ we would like to build a hitogram estimate of the density. If we have $m$ bins of size $h$, we can have the following estimates of risk using leave-one-out cross validation:\n",
    "\n",
    "\\begin{gather*}\n",
    "J(h) = \\int \\hatp^2\\lp x;\\xs{n} \\rp dx - 2 \\int \\hatp \\lp x;\\xs{n} \\rp p(x) dx, \n",
    "\\end{gather*}\n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\int \\hatp^2 \\lp x;\\xs{n} \\rp dx - \\frac{2}{n}\\Sum_{i = 1}^n \\hatp \\lp \\boldsymbol{X}_i ;\\xs{n\\backslash i} \\rp, \\label{eq1}\\tag{1}\n",
    "\\end{gather*}\n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\frac{2}{(n - 1)h} - \\frac{n+1}{(n-1)h}\\Sum_{i = 1}^m \\hatp_j^2,\\quad \\hatp_j = \\frac{n_j}{n}. \\label{eq2}\\tag{2}\n",
    "\\end{gather*}\n",
    "\n",
    "Your task is:\n",
    "1. Build a histogram estimate of the pdf given the sample above, tune bandwidth using leave-one-out CV. Use formula (\\ref{eq2}). Also check out `np.histogram` (1 point)\n",
    "2.\\* Prove that for histograms (\\ref{eq2}) follows from (\\ref{eq1}) (1 bonus point)\n",
    "3. Plot CV estimates $\\hatJ(h)$ that you obtained during selection. Mark optimal bandwidth $h_{cv}$ and report $h_{cv}$ and $\\hatJ(h_{cv})$ (0.5 point)\n",
    "4. Recall the approximation to $MISE$ from Lecture 7, slides 11-13. Assume an Oracle calculated the integral of the squared derivative of the true density for you and the value is $0.0804924$. What will be the approximate optimal bandwidth $h^*$? Find CV estimate of $J$ for this value of bandwidth and compare it to the one found in part 1 (0.5 point)\n",
    "5. `np.histogram` has some built-in methods of selecting bandwidth. Compare some of them with your result and theoretical approximation (again, using CV estimate of $J$) (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will use the same sample, but employ the kernel density estimation method. We can use the same proxy expression to optimise the risk:\n",
    "\\begin{gather*}\n",
    "J(h) = \\int \\hatp^2 \\lp x;\\xs{n} \\rp dx - 2 \\int \\hatp \\lp x;\\xs{n} \\rp p(x) dx\n",
    "\\end{gather*}\n",
    "\n",
    "In out case of kernel estimator, we can obtain:\n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\frac{1}{hn^2}\\Sum_{i = 1}^n\\Sum_{j = 1}^n K^{(2)}\\lp\\frac{x_i - x_j}{h}\\rp + \\frac{2K(0)}{nh}, \\label{eq3}\\tag{3}\n",
    "\\end{gather*}\n",
    "where\n",
    "\\begin{gather*}\n",
    "K^{(2)}(x) = K^*(x) - 2K(x), \\quad K^*(x) = \\int K(x - y) K(y) dy.\n",
    "\\end{gather*}\n",
    "\n",
    "You can use [kernel density estimation from sklearn](http://scikit-learn.org/stable/modules/density.html). Your task is:\n",
    "1. Build a kernel density estimate given the sample from before, tune bandwidth using leave-one-out CV. Try two different kernels of your choice. Use formula \\ref{eq3} for your estimates (1 point)\n",
    "2. Plot CV estimates $\\hatJ(h)$ that you obtained during selection. Mark optimal bandwidth $h_{cv}$ and report $h_{cv}$ and $\\hatJ(h_{cv})$. What kernel worked better in terms of estimated risk? (0.5 point)\n",
    "3. Recall the approximation to $MISE$ from Lecture 7, slide 20. Assume an Oracle calculated the integral of the squared second derivative of the true density for you and the value is $0.127529$. What will be the approximate optimal bandwidth $h^*$? Find CV estimate of $J$ for this value of bandwidth and compare it to the one found in part 1 (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Non-parametric Regression (5.5 pt)\n",
    "\n",
    "In this task you will apply non-parametric regression to airport statistics data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing\n",
    "\n",
    "Load dataset from `airport_operations_report.csv`. Use `General Aviation: Total Operations` as target(dependent) variable and `[Air Carrier Operations, General Aviation: Local Operations]` as covariates(independent variables). Divide $1^{st}$ covariate by `1000`, $2^{nd}$ by `100` and target by `1000`. Make 2D grid for covariates from minimum to maximum values with `100` steps along each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model (1 pt)\n",
    "\n",
    "Perform a nonparametric regression to fit the model $Y = f(x)+\\varepsilon$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict values for created grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_numbers - number of steps in grid: (100, 100)\n",
    "\n",
    "# Your code\n",
    "\n",
    "target_pred = ...\n",
    "target_pred = target_pred.reshape(*step_numbers)\n",
    "grid = grid.reshape(*step_numbers, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is function to visualize 3D surfaces. You can modify it if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface_function(x1, x2, y, minmax_values=None, ax=None):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.gca(projection='3d')\n",
    "\n",
    "        surf = ax.plot_surface(x1, x2, y, cmap=cm.rainbow, linewidth=0.5, alpha=0.5, edgecolor=\"k\")\n",
    "        if minmax_values is not None:\n",
    "            ax.set_xlim3d(*minmax_values[0])\n",
    "            ax.set_ylim3d(*minmax_values[1])\n",
    "    \n",
    "    else:\n",
    "        surf = ax.plot_surface(x1, x2, y, cmap=cm.bwr, linewidth=0.5, alpha=0.1, edgecolor=\"k\")\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot estimated surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target - dependent variable\n",
    "# grid - grid for covariates\n",
    "# sample - covariates\n",
    "\n",
    "ax = plot_surface_function(grid[:, :, 0], grid[:, :, 1], target_pred, minmax_values=np.vstack((sample_min, sample_max)).T)\n",
    "ax.scatter(sample[:, 0], sample[:, 1], target, marker=\"o\", s=25, c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate confidence intervals (2 pt)\n",
    "\n",
    "Construct 95\\% confidence bands for your estimate. To estimate error variance for 1D covariate, you sort your sample and subtract target values in the nearest points. For 2D covariate, you should, for each point, calculate the difference in target values between a point and its nearest neighbor(in euclidian metric). To get power for quantile of standard normal distribution, which is $\\frac{h}{b-a}$ for 1D, estimate it per coordinate and multiply: $\\prod_{i=1}^2 \\frac{h_i}{b_i-a_i}$. For gaussian kernal $h = 3 \\times$ kernel_bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_se(grid, sample, target, h, alpha=0.05):\n",
    "\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "qse = prediction_se(...)\n",
    "qse = qse.reshape(*step_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_surface_function(grid[:, :, 0], grid[:, :, 1], target_pred, minmax_values=np.vstack((sample_min, sample_max)).T)\n",
    "ax.scatter(sample[:, 0], sample[:, 1], target, marker=\"o\", s=25, c=\"r\")\n",
    "ax = plot_surface_function(grid[:, :, 0], grid[:, :, 1], target_pred + qse, ax=ax)\n",
    "ax = plot_surface_function(grid[:, :, 0], grid[:, :, 1], target_pred - qse, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal bandwidth (2 pt)\n",
    "\n",
    "Use cross-validation to estimate the bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_regression(sample, target, h):\n",
    "\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make grid with logarithm step to find best bandwidth, from `0.1` to `10` with `21` steps along each dimension. Visualize obrained results with `plot_surface_function`. Print values for optimal bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot surface with estimated bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion (0.5 pt)\n",
    "\n",
    "Write your conclusions about conducted experiments (2-5 sentence).\n",
    "\n",
    "    Your conclusion here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Model Selection (5.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will select model(choose set of covariates) using AIC criteria and forward/backward stepwise regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing\n",
    "\n",
    "Read dataset `pacn_wrcc.csv`. The target variable is `ly Solar Rad.`, others are covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale covariates to range `[0, 1]` and add bias column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cp Mallow and sample variance (2 pt) \n",
    "\n",
    "Estimate sample variance of error with full set of covariates. Take into account number of dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement estimation of $C_p$ Mallow for given covariates, target and error variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cp(X, y, sigma2):\n",
    "\n",
    "    # Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward stepwise regression (1 pt)\n",
    "\n",
    "Implement forward stepwise regression. Save the order in which covariates are added to feature set and $C_p$ values for those feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_stepwise_regression(X, y, sigma2, alpha=0.):\n",
    "    \n",
    "    # Your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, cp_list = forward_stepwise_regression(X, y, sigma2, alpha)\n",
    "for a, cp_value in enumerate(cp_list, 1):\n",
    "    print(f\"C_p {cp_value:.3f} +{features[:a]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward stepwise regression (1 pt)\n",
    "\n",
    "Implement backward stepwise regression. Save the order in which covariates are removed from feature set and $C_p$ values for those feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_stepwise_regression(X, y, sigma2, alpha=0.):\n",
    "    \n",
    "    # Your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, cp_list = backward_stepwise_regression(X, y, sigma2, alpha)\n",
    "for a, cp_value in enumerate(cp_list, 1):\n",
    "    print(f\"C_p {cp_value:.3f} +{features[:a]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruteforce (1 pt)\n",
    "\n",
    "Find best sets of covariates and their $C_p$ for all sizes of feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruteforce(X, y, sigma2, alpha=0.):\n",
    "    \n",
    "    # Your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, cp_list = bruteforce(X, y, sigma2, alpha)\n",
    "for a, cp_value in enumerate(cp_list):\n",
    "    print(f\"C_p {cp_value:.3f} -{features[a]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion (0.5 pt)\n",
    "\n",
    "Write your conclusions about conducted experiments (2-5 sentence).\n",
    "\n",
    "    Your conclusion here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Local regression (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([-3.        , -2.93939394, -2.87878788, -2.81818182, -2.75757576,\n",
    "       -2.6969697 , -2.63636364, -2.57575758, -2.51515152, -2.45454545,\n",
    "       -2.39393939, -2.33333333, -2.27272727, -2.21212121, -2.15151515,\n",
    "       -2.09090909, -2.03030303, -1.96969697, -1.90909091, -1.84848485,\n",
    "       -1.78787879, -1.72727273, -1.66666667, -1.60606061, -1.54545455,\n",
    "       -1.48484848, -1.42424242, -1.36363636, -1.3030303 , -1.24242424,\n",
    "       -1.18181818, -1.12121212, -1.06060606, -1.        , -0.93939394,\n",
    "       -0.87878788, -0.81818182, -0.75757576, -0.6969697 , -0.63636364,\n",
    "       -0.57575758, -0.51515152, -0.45454545, -0.39393939, -0.33333333,\n",
    "       -0.27272727, -0.21212121, -0.15151515, -0.09090909, -0.03030303,\n",
    "        0.03030303,  0.09090909,  0.15151515,  0.21212121,  0.27272727,\n",
    "        0.33333333,  0.39393939,  0.45454545,  0.51515152,  0.57575758,\n",
    "        0.63636364,  0.6969697 ,  0.75757576,  0.81818182,  0.87878788,\n",
    "        0.93939394,  1.        ,  1.06060606,  1.12121212,  1.18181818,\n",
    "        1.24242424,  1.3030303 ,  1.36363636,  1.42424242,  1.48484848,\n",
    "        1.54545455,  1.60606061,  1.66666667,  1.72727273,  1.78787879,\n",
    "        1.84848485,  1.90909091,  1.96969697,  2.03030303,  2.09090909,\n",
    "        2.15151515,  2.21212121,  2.27272727,  2.33333333,  2.39393939,\n",
    "        2.45454545,  2.51515152,  2.57575758,  2.63636364,  2.6969697 ,\n",
    "        2.75757576,  2.81818182,  2.87878788,  2.93939394,  3.        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
    "       1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a sample of pairs $(X_i, Y_i)$. The output variable appers to be binary. We know a model for this type of data: logistic regression. In this model we assume the log-odds of the Bernoulli output variable to be a linear function:\n",
    "$$\\mathbb{P}(Y_i = 1 | X_i = x_i) = p(x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}$$\n",
    "Lets fit a standard logistic regression to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty='none')\n",
    "logreg.fit(X.reshape(-1, 1), Y)\n",
    "p_pred_global = logreg.predict_proba(X.reshape(-1, 1))[:, 1]\n",
    "print(f\"Coefficients of the fitted logistic regression model: b0={logreg.intercept_[0]}, b1={logreg.coef_[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y, '*', label='Observations')\n",
    "plt.plot(X, p_pred_global, label='Logictic regression')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dependence of the outcome on $x$ looks more complex than estimated using logistic regression. We will try to improve it with local regression methods instead.\n",
    "Instead of a global model, we will approximate conditinal probability of the positive class in a neighbourhood of $x$ with:\n",
    "$$p(u) \\approx \\frac{e^{\\beta_0 + \\beta_1 (u-x) }}{1 + e^{\\beta_0 + \\beta_1 (u-x)}}, $$\n",
    "for $u$ close to $x$. The coefficients in this case will depend on $x$. To find them, we will have to fit a (slightly different) model at each new query point $x_{new}$ (point where we want to predict $Y$). We will also need to introduce a notion of 'closenes' of points across $x$ - for this we can use our familiar *kernel functions*. Bringing everything together, we introduce the following local loglikelihood for our proposed model:\n",
    "$$\\ell_x(\\beta) = \\sum\\limits_{i=1}^{n} K\\left(\\frac{x-X_i}{h}\\right)\\ell\\left(Y_i, \\beta_0 + \\beta_1 \\left(X_i - x\\right)\\right) = \\sum\\limits_{i=1}^{n} K\\left(\\frac{x-X_i}{h}\\right) \\left(Y_i \\left(\\beta_0 + \\beta_1 \\left(X_i - x\\right)\\right) - \\log \\left( 1 + e^{\\beta_0 + \\beta_1 \\left(X_i - x\\right)} \\right) \\right),$$\n",
    "$$\\ell(y, z) = yz - \\log \\left(1 + e^z \\right).$$\n",
    "Here $\\ell(y, z)$ is a log-likelohood for a single Bernoulli with log-dds ratio $z = \\log\\frac{p}{1-p}$. The local log-likelihood $\\ell_x(\\beta)$ should be optimized numerically to each new point $x$ to obtain $\\widehat{\\beta}(x)=(\\widehat{\\beta_0}(x), \\widehat{\\beta_1}(x))$. Then, we can predict $Y$ using the logistic model:\n",
    "$$ \\mathbb{P}(Y=1 | X=x) = \\frac{e^{\\beta_0(x)}}{1 + e^{\\beta_0(x)}}.$$\n",
    "Your task is:\n",
    "1. Implement the proposed local logistic regression approach using Gaussian kernel. *Hint*: look at additional parameters of the `fit` method of [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) from `sklearn` package (2 points)\n",
    "2. Select optimal bandwidth using leave-one-out log-likelihood cross-validation: $\\ell_{cv} = \\sum_i \\ell(Y_i, \\widehat{z}_{-i}(x_i))$, where $\\widehat{z}_{-i}$ is the estimated log-odds ratio without $i$th sample (3 points)\n",
    "3. Now ignore that the output variable is binary. Compare previous results with local linear kernel regression method using `KernelReg` from `statsmodels`. Use leave-one out squared error cross-validation to select the bandwidth (1 bonus point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Local Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Local Linear regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
